{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = make_moons(10000)\n",
    "Y = Y.reshape(-1,1)\n",
    "scale(X, axis=0, copy=False)\n",
    "m,n = X.shape\n",
    "X_bias = np.c_[np.ones((m,1)),X]\n",
    "X_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X_all, y_all, **kwargs):\n",
    "    tf.reset_default_graph()\n",
    "    m,n = X_all.shape\n",
    "    bs = kwargs['batch_size'] #for brevity\n",
    "    \n",
    "    from datetime import datetime\n",
    "    now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    root_logdir = 'tf_logs'\n",
    "    logdir = '{}/moons-{}'.format(root_logdir, now)\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,n), name='X')\n",
    "    y = tf.placeholder(tf.float32, shape=(None,1), name='y')\n",
    "    \n",
    "    n_batches = int(np.ceil(m/kwargs['batch_size']))\n",
    "    \n",
    "    theta = tf.Variable(tf.random_uniform([n,1],-1.0,1.0), name='theta')\n",
    "    ypred = tf.matmul(X, theta, name='ypred')\n",
    "    error = ypred-y\n",
    "    mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=0.01, momentum=0.9)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "    mse_summary = tf.summary.scalar('MSE', mse)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(kwargs['n_epochs']):\n",
    "            for index in range(n_batches):\n",
    "                X_batch, y_batch = X_all[index*bs:(index+1)*bs,:], y_all[index*bs:(index+1)*bs,:]\n",
    "                sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "                if index % 10 == 0:\n",
    "                    summary_str = mse_summary.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "                    step = epoch*n_batches + index\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                    print('writing %d'%(step))\n",
    "                    \n",
    "        best_theta = theta.eval()\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 0\n",
      "writing 10\n",
      "writing 20\n",
      "writing 30\n",
      "writing 40\n",
      "writing 50\n",
      "writing 60\n",
      "writing 70\n",
      "writing 80\n",
      "writing 90\n",
      "writing 100\n",
      "writing 110\n",
      "writing 120\n",
      "writing 130\n",
      "writing 140\n",
      "writing 150\n",
      "writing 160\n",
      "writing 170\n",
      "writing 180\n",
      "writing 190\n",
      "writing 200\n",
      "writing 210\n",
      "writing 220\n",
      "writing 230\n",
      "writing 240\n",
      "writing 250\n",
      "writing 260\n",
      "writing 270\n",
      "writing 280\n",
      "writing 290\n",
      "writing 300\n",
      "writing 310\n",
      "writing 320\n",
      "writing 330\n",
      "writing 340\n",
      "writing 350\n",
      "writing 360\n",
      "writing 370\n",
      "writing 380\n",
      "writing 390\n",
      "writing 400\n",
      "writing 410\n",
      "writing 420\n",
      "writing 430\n",
      "writing 440\n",
      "writing 450\n",
      "writing 460\n",
      "writing 470\n",
      "writing 480\n",
      "writing 490\n",
      "writing 500\n",
      "writing 510\n",
      "writing 520\n",
      "writing 530\n",
      "writing 540\n",
      "writing 550\n",
      "writing 560\n",
      "writing 570\n",
      "writing 580\n",
      "writing 590\n",
      "writing 600\n",
      "writing 610\n",
      "writing 620\n",
      "writing 630\n",
      "writing 640\n",
      "writing 650\n",
      "writing 660\n",
      "writing 670\n",
      "writing 680\n",
      "writing 690\n",
      "writing 700\n",
      "writing 710\n",
      "writing 720\n",
      "writing 730\n",
      "writing 740\n",
      "writing 750\n",
      "writing 760\n",
      "writing 770\n",
      "writing 780\n",
      "writing 790\n",
      "writing 800\n",
      "writing 810\n",
      "writing 820\n",
      "writing 830\n",
      "writing 840\n",
      "writing 850\n",
      "writing 860\n",
      "writing 870\n",
      "writing 880\n",
      "writing 890\n",
      "writing 900\n",
      "writing 910\n",
      "writing 920\n",
      "writing 930\n",
      "writing 940\n",
      "writing 950\n",
      "writing 960\n",
      "writing 970\n",
      "writing 980\n",
      "writing 990\n",
      "writing 1000\n",
      "writing 1010\n",
      "writing 1020\n",
      "writing 1030\n",
      "writing 1040\n",
      "writing 1050\n",
      "writing 1060\n",
      "writing 1070\n",
      "writing 1080\n",
      "writing 1090\n",
      "writing 1100\n",
      "writing 1110\n",
      "writing 1120\n",
      "writing 1130\n",
      "writing 1140\n",
      "writing 1150\n",
      "writing 1160\n",
      "writing 1170\n",
      "writing 1180\n",
      "writing 1190\n",
      "writing 1200\n",
      "writing 1210\n",
      "writing 1220\n",
      "writing 1230\n",
      "writing 1240\n",
      "writing 1250\n",
      "writing 1260\n",
      "writing 1270\n",
      "writing 1280\n",
      "writing 1290\n",
      "writing 1300\n",
      "writing 1310\n",
      "writing 1320\n",
      "writing 1330\n",
      "writing 1340\n",
      "writing 1350\n",
      "writing 1360\n",
      "writing 1370\n",
      "writing 1380\n",
      "writing 1390\n",
      "writing 1400\n",
      "writing 1410\n",
      "writing 1420\n",
      "writing 1430\n",
      "writing 1440\n",
      "writing 1450\n",
      "writing 1460\n",
      "writing 1470\n",
      "writing 1480\n",
      "writing 1490\n",
      "writing 1500\n",
      "writing 1510\n",
      "writing 1520\n",
      "writing 1530\n",
      "writing 1540\n",
      "writing 1550\n",
      "writing 1560\n",
      "writing 1570\n",
      "writing 1580\n",
      "writing 1590\n",
      "writing 1600\n",
      "writing 1610\n",
      "writing 1620\n",
      "writing 1630\n",
      "writing 1640\n",
      "writing 1650\n",
      "writing 1660\n",
      "writing 1670\n",
      "writing 1680\n",
      "writing 1690\n",
      "writing 1700\n",
      "writing 1710\n",
      "writing 1720\n",
      "writing 1730\n",
      "writing 1740\n",
      "writing 1750\n",
      "writing 1760\n",
      "writing 1770\n",
      "writing 1780\n",
      "writing 1790\n",
      "writing 1800\n",
      "writing 1810\n",
      "writing 1820\n",
      "writing 1830\n",
      "writing 1840\n",
      "writing 1850\n",
      "writing 1860\n",
      "writing 1870\n",
      "writing 1880\n",
      "writing 1890\n",
      "writing 1900\n",
      "writing 1910\n",
      "writing 1920\n",
      "writing 1930\n",
      "writing 1940\n",
      "writing 1950\n",
      "writing 1960\n",
      "writing 1970\n",
      "writing 1980\n",
      "writing 1990\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'learning_rate':0.01, \n",
    "        'batch_size': 100,\n",
    "         'n_epochs':20}\n",
    "\n",
    "logistic_regression(X_bias,Y,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
